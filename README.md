# kaggle_abstraction_classification
Project 2 for McGill COMP551 Applied Machine Learning

# Attention:
Incorrectly labeled samples (i.e. samples with a label 'category') have been removed from train_in.csv and train_out.csv manually. Therefore use the train_in.csv and train_out.csv downloaded from HERE! (train_in.csv have to be cleaned by yourself with excel filtering or any other approach because it's too big to upload...:P)

# What I Have Done:
Multinomial.py: 2 classes of multinomial and bernoulli naive bayes implemented without scikit-learn.

feature_extraction.py: class for features set-up. now considering bag-of-words with features more than 1000.

try_scikit_package.py: cross-validation for pakages from scikit-learn and Multinomial.py.

Kaggle_test.py: code for generating ouputs should be uploaded to Kaggle for test. !!!REMEMBER to DELETE the first column before upload!!!

Many Thanks to scikit-learn XD

# What I Want to Do But Have No Time to Consider:
Implement manual works in code

BernoulliNB's performance is very very unacceptable!

K-fold cross-validation (for what?)

Find the optimal number of features
